---
title: "Flexible variable selection in the presence of missing data"
author: "Brian D. Williamson, PhD<br> <span style= 'font-size: 75%;'> Kaiser Permanente Washington Health Research Institute </span>"
date: "13 June, 2022 <br> <a href = 'https://bdwilliamson.github.io/' style = 'color: white;'>https://bdwilliamson.github.io/</a>"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      titleSlideClass: ["center", "middle"]
      highlightStyle: tomorrow-night-blue
      highlightLanguage: rmarkdown
      highlightLines: false
      slideNumberFormat: '%current%'
      countIncrementalSlides: false
---

```{r xaringan-setup, include=FALSE, warning=FALSE}
library("xaringanthemer")

extra_css <- list(
  ".tiny" = list(`font-size` = "60%"),
  ".small" =  list(`font-size` = "80%"),
  ".large" =  list(`font-size` = "150%"),
  ".huge" =  list(`font-size` = "300%"),
  "ul li" = list(`margin-bottom` = "10px"),
  ".gray" = list(color = "#C0C0C0"),
  ".red" = list(color = "#FF0000"),
  ".green" = list(color = "#097969"),
  ".blue1" = list(color = "#3AABBE"),
  ".blue2" = list(color = "#2A6D90"),
  ".blue3" = list(color = "#446874"),
  ".purple" = list(color = "#624474"),
  ".mutedred" = list(color = "#745344"),
  "a" = list(color = "#3AABBE"),
  "a:hover" = list("text-decoration" = "underline")
)
# set up fonts
style_mono_accent(
  base_color = "#446874",
  header_font_google = google_font("DM Sans"),
  text_font_google   = google_font("DM Sans", "400", "400i"),
  code_font_google   = google_font("Courier Prime"),
  extra_css = extra_css
)

xaringanExtra::use_tile_view()
```

<style type="text/css">
.remark-slide-content {
    font-size: 20px
    header-h2-font-size: 1.75rem;
}
</style>

## Acknowledgments

This work was done in collaboration with:
```{r acknowledgments, echo = FALSE, fig.show = "hold", out.width = "75%", fig.align = "center"}
knitr::include_graphics(c(
    "img/people1.PNG"
  ))
```

Many thanks for support from NIGMS!

---

## Challenge: variable selection with missing data

Goals of variable selection:
* identify a .green[parsimonious] set of features
* achieve .blue1[desired level of performance]

--

How do we measure performance?
* .red[error rate control]: not too many unimportant variables were selected
* .blue2[predictiveness]: how well our parsimonious set predicts the outcome

--

Identifying and assessing sets of variables is .red[complex] with missing data

---

## Challenge: variable selection with missing data

Common approaches to handling missing data:
* model the likelihood .small[(Little & Schluchter, 1985; Garcia et al., 2010)];
* use inverse probability weighting .small[(Tsiatis, 2007; Bang & Robins, 2005; Johnson et al., 2008)];
* use multiple imputations .small[(Long & Johnson, 2015; Zhao & Long, 2017; Liu et al., 2019)]

--

Variable selection with missing data: uses "complete" data

--

Selection is typically based on a generalized linear model:
* lasso .small[(Tibshirani, 1996)] or SCAD .small[(Fan and Li, 2001)]
* knockoffs .small[(Barber and Candès, 2015)]
* stability selection .small[(Meinshausen and Bühlmann, 2010)]

--

Potential issue: .red[model misspecification]

---

## Flexible variable selection

**What is flexible variable selection?**
* Allow specification of flexible (i.e., nonlinear) algorithms
* e.g., trees, forests, ensembles

--

.green[Robustness to model misspecification] depends on the algorithms considered

--

Our work seeks to:
* combine multiple imputation with flexible variable selection
* provide error rate control for intrinsic selection procedures

---

## Variable importance in selection

A natural procedure for flexible selection uses .green[variable importance]:

--

Choose and estimate a measure of variable importance 
* .blue1[extrinsic importance] .small[(e.g., Breiman, 2001)]
* .blue1[intrinsic importance] .small[(e.g., Williamson & Feng, 2020)]

--

Select variables based on .blue2[decreasing importance]

--

Extrinsic importance $\rightarrow$ extrinsic selection

Intrinsic importance $\rightarrow$ intrinsic selection

---

## Super Learner extrinsic selection

Setup:
* data unit $(X, Y) \sim P_0$ with outcome $Y$ and covariate $X := (X_1, X_2, \ldots, X_p)$
* goal: measure extrinsic importance of $(X_j: j \in \{1, \ldots, p\})$

--

Relying on a single algorithm: could be .red[misspecified]

--

Instead fit a .blue1[Super Learner ensemble] with $L$ learners .small[(van der Laan et al., 2007)]

--

Super Learner returns a set of weights $\{w_\ell\}_{\ell = 1}^L$ such that:
  * $0 \leq w_\ell \leq 1$ for all $\ell$, $\sum_{\ell = 1}^L w_\ell = 1$
  * the final ensemble minimizes a cross-validated loss function

---

## Super Learner extrinsic selection

Provided a threshold $\kappa \geq 1$:
1. Fit a Super Learner with $L$ candidate learners, get $\{w_\ell\}_{\ell = 1}^L$;

--

2. Obtain ranked algorithm-specific importance $\{r_{j,\ell}\}_{j=1}^p$;

--

3. Compute $r_{j,L} := \sum_{\ell=1}^L w_\ell r_{j,\ell}$;

--

4. Select all variables with index in $\{j \in \{1, \ldots, p\}: r_{j,L} < \kappa\}$

--

Can also be embedded within .blue1[stability selection]

---

## Intrinsic variable selection

One possible measure: SPVIM .small[(Williamson & Feng, 2020)]
$$\psi_{0,j} := \sum_{s \in \{1, \ldots, p\} \setminus \{j\}} \frac{1}{p}\binom{p - 1}{\lvert s \rvert}^{-1}\{V(f_{0,s\cup\{j\}}, P_0) - V(f_{0,s}, P_0)\},$$ where:
* $V$ is a user-specified measure of 'predictiveness' (e.g., $R^2$)
* $f_{0,s} \in \text{arg max}_{f \in \mathcal{F}} V(f, P_0)$ for class of candidate functions $\mathcal{F}$;


--

<blockquote> If \(\psi_{0,j} = 0\), then \(X_j\) .red[does not improve] population prediction potential when added to .green[any subset] of the remaining features </blockquote>

--

&zwj;Idea: test variable importance for selection, i.e., test $$H_0: \psi_{0,j} = 0 \text{ vs } H_1: \psi_{0,j} > 0 \text{ for each } j \in \{1, \ldots, p\}$$

---

## Intrinsic variable selection

How can we select variables based on $\psi_{0,j}$?

--

1. Estimate $\psi_0 = \{\psi_{0,j}\}_{j=1}^p$, obtain p-values $p_{n,j}$ for each test of zero importance

--

3. Compute .blue1[adjusted] p-values $\tilde{p}_{n,j}$ to control the .blue2[family-wise error rate] (FWER)

--

4. Set $S_n(\alpha) = \{j \in \{1, \ldots, p\}: \tilde{p}_{n,j} < \alpha\}$

--

5. For $k \in \{0, \ldots, p - \lvert S_n(\alpha)\rvert\}$, determine .blue1[augmentation set] 
$$A_n(k, \alpha) = \{s \subseteq S_n^c(\alpha): \tilde{p}_{n,\ell} \leq \tilde{p}_{n,(k)} \text{ for all } \ell \in s\}$$ 
  
  (if $k = 0$, $A_n(k, \alpha) = \emptyset$)

--

6. Final set of selected variables: $S_n^+(k, \alpha) = S_n(\alpha) \cup A_n(k, \alpha)$

---

## Intrinsic variable selection

The .blue1[augmentation set] allows for error control using a tuning parameter $k \in \{0, \ldots, p - \lvert S_n(\alpha)\rvert\}$

--

Examples of more general error rates: 

* .blue2[generalized FWER]: probability of making more than $k + 1$ type I errors

* .blue2[proportion of false positives] among the rejected variables greater than $q(k) \in (0, 1)$

* .blue2[false discovery rate]

--

$k$ can be determined to .blue1[control one of these error rates]! 

And the resulting procedure is .blue1[persistant]

.small[ [Details in Williamson and Huang (2022)] ]

---

## Extrinsic selection with missing data

Previous approaches all work with complete data

--

Several options for missing data, including:
1. Embed within bootstrapped procedure .small[(Long & Johnson, 2015)]

--

2. Harmonize after $M$ imputations .small[(Heymans et al., 2007)]

---

## Intrinsic selection with missing data

How can we select variables based on $\psi_{0,j}$ with missing data? 

--

1. Multiply impute the data

--

2. Estimate $\psi_0$ on each imputed dataset

--

3. Use Rubin's rules: combine estimates, standard errors; obtain p-values

--

4. Proceed as with complete data

---

## Numerical results

Selection procedures:
* lasso 
  * lasso + stability selection .small[ [Meinshausen and Buhlmann (2010)] ] 
  * lasso + knockoffs .small[ [Barber and Candes (2015)] ]

--
* extrinsic selection 
* intrinsic selection (SPVIM with augmentation)

--

Two settings: (binary outcome, continuous features, varying missing data)
* linear outcome-feature relationship (setting 1)
  * 6 important features (some very important, some weakly important)
  * $p \in \{30, 500\}$

--

* nonlinear outcome-feature relationship, correlated features (setting 2)
  * 3 important features (all equally weakly important)
  * $p = 6$
  
---

## Numerical results: setting 1

```{r results-a-auc, echo = FALSE, fig.width = 10 / 300, fig.height = 4 / 300, dpi = 300, fig.show = "hold", fig.align = "center"}
knitr::include_graphics(c(
    "img/binomial-probit-linear-normal-nested_talks-auc.png"
  ))
```

---

## Numerical results: setting 1

```{r results-a-sens-spec, echo = FALSE, fig.show = "hold", fig.width = 10 / 300, fig.height = 4 / 300, dpi = 300,  fig.align = "center"}
knitr::include_graphics(c(
    "img/binomial-probit-linear-normal-nested_talks-sens-spec.png"
  ))
```

---


## Numerical results: setting 2

```{r results-b-auc, echo = FALSE, fig.show = "hold", fig.width = 10 / 300, fig.height = 4 / 300, dpi = 300,  fig.align = "center"}
knitr::include_graphics(c(
    "img/nonlinear-normal-correlated_talks-auc.png"
  ))
```

---

## Numerical results: setting 2

```{r results-b-sens-spec, echo = FALSE, fig.show = "hold", fig.width = 10 / 300, fig.height = 4 / 300, dpi = 300,  fig.align = "center"}
knitr::include_graphics(c(
    "img/nonlinear-normal-correlated_talks-sens-spec.png"
  ))
```

---


## Classifying pancreatic cysts

Cross-validated AUC of each procedure for selecting sets:
```{r data-results, echo = FALSE, fig.show = "hold", fig.width = 10 / 300, fig.height = 5 / 300, dpi = 300,  fig.align = "center"}
knitr::include_graphics(c(
    "img/data-analysis_v2.png"
  ))
```

---

## Perspective and closing thoughts

* Use ensemble to achieve more stable extrinsic selection
* Error rate control possible with intrinsic selection
* Computation time can be .red[quite large] depending on flexibility
* Choice of selection procedure likely depends on trade-off between:
  * computation time
  * desired sensitivity/specificity
* Important to use flexible final prediction algorithm regardless

* R package [`flevr`](https://github.com/bdwilliamson/flevr)

`r icons::fontawesome('github')` https://github.com/bdwilliamson | `r icons::fontawesome('globe')` https://bdwilliamson.github.io

---

## References

<style type="text/css">div.smalllist
ul li {
  margin-bottom: 4px;
  line-height: 11px;
}
</style>

.small[Many thanks for tuning in!]
.pull-left[
.smalllist[
* .tiny[Barber, R. and E.J. Candès (2015). Controlling the false discovery rate via knockoffs. *Annals of Statistics*.]
* .tiny[Bang, H. and J. Robins (2005). Doubly robust estimation in missing data and causal inference models. *Biometrics*.]
* .tiny[Breiman, L. (2001). Random forests. *Machine Learning*.]
* .tiny[Dudoit, S., J. Shaffer, and J. Boldrick (2003). Multiple hypothesis testing in microarray experiments. *Statistical Science*.]
* .tiny[Dudoit, S. and M.J. van der Laan (2008). *Multiple testing procedures with applications to genomics*. Springer Science & Business Media.]
* .tiny[Fan, J. and R. Li (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. *Journal of the American Statistical Association*.]
* .tiny[ Garcia, R., J. Ibrahim, and H. Zhu (2010). Variable selection for regression models with missing data. *Statistica Sinica*. ]
* .tiny[Heymans, M., S. van Buuren, D. Knol, et al. (2007). Variable selection under multiple imputation using the bootstrap in a prognostic study. *BMC Medical Research Methodology*.]
* .tiny[Johnson, B., D. Lin, and D. Zeng (2008). Penalized estimating functions and variable selection in semiparametric regression models. *Journal of the American Statistical Association*.]
]
]
.pull-right[
.smalllist[
* .tiny[ Little, R. and M. Schluchter (1985). Maximum likelihood estimation for mixed continuous and categorical data with missing values. *Biometrika*.]
* .tiny[Liu, L., Y. Qiu, L. Natarajan, and K. Messer (2019). Imputation and post-selection inference in models with missing data: an application to colorectal cancer surveillance guidelines. *Annals of Applied Statistics*.]
* .tiny[Long, Q. and B. Johnson (2015). Variable selection in the presence of missing data: resampling and imputation. *Biostatistics*.]
* .tiny[Meinshausen, N. and P. Bühlmann (2010). Stability selection. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*.]
* .tiny[Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*.]
* .tiny[Tsiatis, A. (2007). *Semiparametric theory and missing data*. Springer Science & Business Media.]
* .tiny[van der Laan, M.J., E. Polley, and A. Hubbard (2007). Super learner. *Statistical Applications in Genetics and Molecular Biology*.]
* .tiny[Williamson, B.D. and J. Feng (2020). Efficient nonparametric statistical inference on population feature importance using Shapley values. In *Proceedings of the 37th International Conference on Machine Learning*.]
* .tiny[Williamson, B.D. and Y. Huang (2022). Flexible variable selection in the presence of missing data. *arXiv*.]
* .tiny[Zhao, Y. and Q. Long (2017). Variable selection in the presence of missing data: imputation-based methods. *Wiley Interdisciplinary Reviews: Computational Statistics*.]
]
]